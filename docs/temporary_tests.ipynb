{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7791ac88",
   "metadata": {},
   "source": [
    "# Temporary tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d9e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5729689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_text(num_phrases):\n",
    "    phrases = []\n",
    "    for i in range(1, num_phrases + 1):\n",
    "        phrase = f\"Phrase {i}.\"\n",
    "        phrases.append(phrase)\n",
    "\n",
    "    fake_text = \" \".join(phrases)\n",
    "    return fake_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e22bf312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase 1. Phrase 2. Phrase 3. Phrase 4. Phrase 5.  \n",
      "['Phrase 1', 'Phrase 2', 'Phrase 3', 'Phrase 4', 'Phrase 5']\n"
     ]
    }
   ],
   "source": [
    "DEMO_TEXT = generate_fake_text(20)\n",
    "print(f\"\"\"{DEMO_TEXT[:50]} \\n{DEMO_TEXT.split(\". \")[:5]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a693de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase 1. Phrase 2. Phrase 3. Phrase 4. Phrase 5. Phrase 6. Phrase 7. Phrase 8. Phrase 9. Phrase 10. Phrase 11. Phrase 12. Phrase 13. Phrase 14. Phrase 15. Phrase 16. Phrase 17. Phrase 18. Phrase 19. Phrase 20.\n",
      "Phrase 1. Phrase 2. Phrase 3. Phrase 4. Phrase 7. Phrase 9. Phrase 10. Phrase 12. Phrase 13. Phrase 14. Phrase 15. Phrase 16. Phrase 17. Phrase 18. Phrase 19.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CASE 1: REMOVE INDEPENDENTLY A RANDOM NUMBER OF SEQUENCES (see branch: stb_fragment_ays1).\n",
    "def fragment(text: str, frag_rate: float) -> str:\n",
    "    \"\"\"Fragment a text by removing sentences randomly.\n",
    "    # Sentences are removed independently from each other.\n",
    "    # Equivalent to add a `delete` method to the Sentence class with a \n",
    "    # ProbabilisticConfig object as parameter (law: `Bernouilli`, rate: float).\n",
    "\n",
    "    Args:\n",
    "        frag_rate (float): The probability of sentence deletion.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with sentences deleted.\n",
    "    \"\"\"\n",
    "    if not 0 <= frag_rate <= 1:\n",
    "        raise ValueError(\"Probability larger than one or is negative.\")\n",
    "    \n",
    "    sentences = text.split(\".\")\n",
    "    nbr_to_delete = int(frag_rate * len(sentences))\n",
    "    if nbr_to_delete > 0:\n",
    "        deletion_indices = random.sample(range(len(sentences)),\n",
    "                                            nbr_to_delete)\n",
    "        for index in sorted(deletion_indices, reverse=True):\n",
    "            del sentences[index]\n",
    "    return \".\".join(sentences)\n",
    "\n",
    "print(fragment(DEMO_TEXT, 0))\n",
    "print(fragment(DEMO_TEXT, 0.25))\n",
    "print(fragment(DEMO_TEXT, 1))\n",
    "# print(fragment(DEMO_TEXT, 2)) # Error Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98e7b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 Error\n",
      "0.0 - 20 sentences - Phrase 1. Phrase 2. Phrase 3. Phrase 4. Phrase 5. Phrase 6. Phrase 7. Phrase 8. Phrase 9. Phrase 10. Phrase 11. Phrase 12. Phrase 13. Phrase 14. Phrase 15. Phrase 16. Phrase 17. Phrase 18. Phrase 19. Phrase 20.\n",
      "0.25 - 15 sentences - Phrase 1. Phrase 2. Phrase 3. Phrase 9. Phrase 10. Phrase 11. Phrase 12. Phrase 13. Phrase 14. Phrase 15. Phrase 16. Phrase 17. Phrase 18. Phrase 19. Phrase 20.\n",
      "0.5 - 10 sentences - Phrase 1. Phrase 2. Phrase 3. Phrase 4. Phrase 5. Phrase 6. Phrase 7. Phrase 8. Phrase 19. Phrase 20.\n",
      "0.75 - 4 sentences - Phrase 1. Phrase 2. Phrase 3. Phrase 4. Phrase 5\n",
      "1.0 - 0 sentences - \n",
      "2.0 Error\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# CASE2: REMOVING ONE SEQUENCE OF SENTENCES.   \n",
    "def fragment_sequence(text: str, frag_rate: float, punc: str = \".\") -> str:\n",
    "    \"\"\"\n",
    "    Fragment a text by randomly removing ONE sequence of sentences whose\n",
    "    length is expressed as a percentage of the text length.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be fragmented.\n",
    "        frag_rate (float): The rate of sentence deletion (0 <= frag_rate <= 1).\n",
    "            Represents the percentage of sentences to be deleted.\n",
    "        punc (str, optional): The punctuation used to split the text into sentences.\n",
    "            Default is period/full stop (\".\").\n",
    "    \n",
    "    Returns:\n",
    "        str: The fragmented text with sentences deleted.\n",
    "    \"\"\"\n",
    "    if not 0 <= frag_rate <= 1:\n",
    "        raise ValueError(\"Probability or rate larger than one or is negative.\")\n",
    "    # Split the text into sentences using the specified punctuation.\n",
    "    sentences = text.split(punc)\n",
    "    # Calculate the total number of sentences and the number of sentences to delete.\n",
    "    num_sentences = len(sentences)\n",
    "    num_sentences_to_delete = round(num_sentences * frag_rate)\n",
    "    # Choose a random starting point for the sequence of sentences to be deleted.\n",
    "    start_frag_location = min(random.choice(range(num_sentences)),\n",
    "                              num_sentences - num_sentences_to_delete)\n",
    "    # Determine the ending point of the fragment to be deleted.\n",
    "    end_frag_location = start_frag_location + num_sentences_to_delete\n",
    "    # Delete the selected sequence of sentences from the list.\n",
    "    del sentences[start_frag_location:end_frag_location]\n",
    "\n",
    "    # Join the remaining sentences back into a single text.\n",
    "    fragmented_text = punc.join(sentences)\n",
    "\n",
    "    return fragmented_text\n",
    "\n",
    "for rate in [-1, 0.0, 0.25, 0.5, 0.75, 1.0, 2.0]:\n",
    "    try:\n",
    "        res = fragment_sequence(DEMO_TEXT, rate)\n",
    "        length = len(res.split(\".\")) - 1 # approx\n",
    "        print(f\"{rate} - {length} sentences - {res}\")\n",
    "    except:\n",
    "        print(rate, \"Error\")\n",
    "        pass\n",
    "# print(fragment(DEMO_TEXT, 2)) # Error Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ef631104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[6, 9]\n"
     ]
    }
   ],
   "source": [
    "# # CASE 3: REMOVING `MANY` (A RANDOM NUMBER OF) SEQUENCES OF SENTENCES.\n",
    "# ## Case 3.1: simple case \n",
    "# ### * constant length of fragment locations\n",
    "# ### * uniform repartition of fragment locations whithin the document\n",
    "# def fragment_sequences(text: str, frag_rate: float, punc: str = \".\") -> str:\n",
    "#     \"\"\"\n",
    "#     Fragment a text by randomly removing MANY sequences of sentences. The total\n",
    "#     number of sentences to be deleted in the text is the sum of the sequences length and \n",
    "#     is expressed as a percentage of the text length.\n",
    "\n",
    "#     Args:\n",
    "#         text (str): The input text to be fragmented.\n",
    "#         frag_rate (float): The rate of sentence deletion (0 <= frag_rate <= 1).\n",
    "#             Represents the percentage of sentences to be deleted.\n",
    "#         punc (str, optional): The punctuation used to split the text into sentences.\n",
    "#             Default is period/full stop (\".\").\n",
    "    \n",
    "#     Returns:\n",
    "#         str: The fragmented text with sentences deleted.\n",
    "#     \"\"\"\n",
    "#     if not 0 <= frag_rate <= 1:\n",
    "#         raise ValueError(\"Probability or rate larger than one or is negative.\")\n",
    "#     # Split the text into sentences using the specified punctuation.\n",
    "#     sentences = text.split(punc)\n",
    "#     # Calculate the total number of sentences and the number of sentences to delete.\n",
    "#     num_sentences = len(sentences)\n",
    "#     num_sentences_to_delete = round(num_sentences * frag_rate)\n",
    "#     # Choose a random starting point for the sequence of sentences to be deleted.\n",
    "#     start_frag_location = min(random.choice(range(num_sentences)),\n",
    "#                               num_sentences - num_sentences_to_delete)\n",
    "#     # Determine the ending point of the fragment to be deleted.\n",
    "#     end_frag_location = start_frag_location + num_sentences_to_delete\n",
    "#     # Delete the selected sequence of sentences from the list.\n",
    "#     del sentences[start_frag_location:end_frag_location]\n",
    "\n",
    "#     # Join the remaining sentences back into a single text.\n",
    "#     fragmented_text = punc.join(sentences)\n",
    "\n",
    "#     return fragmented_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303ffee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from stemmabench.stemma_generator import Stemma\n",
    "from stemmabench.config_parser import StemmaBenchConfig\n",
    "from loguru import logger\n",
    "# Set logging level to info\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d96529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = StemmaBenchConfig(**{\n",
    "    \"meta\": {\n",
    "      \"language\": \"eng\"  \n",
    "    },\n",
    "    \"stemma\": {\n",
    "        \"depth\": 2,\n",
    "        \"width\": {\n",
    "            \"law\": \"Uniform\",\n",
    "            \"min\": 2,\n",
    "            \"max\": 3\n",
    "        },\n",
    "        \"fragmentation\": {\n",
    "            \"law\": \"Bernouilli\",\n",
    "            \"rate\": 1\n",
    "        }\n",
    "    },\n",
    "    \"variants\": {\n",
    "        \"sentences\": {\n",
    "            \"duplicate\": {\n",
    "                \"args\": {\"nbr_words\": 1},\n",
    "                \"law\": \"Bernouilli\",\n",
    "                \"rate\": 0.05\n",
    "            },\n",
    "            # \"delete\": {\n",
    "            #     \"law\": \"Bernouilli\",\n",
    "            #     \"rate\": 0.01\n",
    "            # }\n",
    "        },\n",
    "        \"words\": {\n",
    "            \"synonym\": {\n",
    "                \"law\": \"Bernouilli\",\n",
    "                \"rate\": 0.05,\n",
    "                \"args\": {}\n",
    "            },\n",
    "            \"mispell\": {\n",
    "                \"law\": \"Bernouilli\",\n",
    "                \"rate\": 0.001,\n",
    "                \"args\": {}\n",
    "            },\n",
    "            \"omit\": {\n",
    "                \"law\": \"Bernouilli\",\n",
    "                \"rate\": 0.001,\n",
    "                \"args\": {}\n",
    "            }\n",
    "        },\n",
    "        \"texts\": {\n",
    "            \"fragment\": {\n",
    "                \"rate\": 0.001,\n",
    "                # \"spread\":,\n",
    "                # sentences_weigths:, \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dfc61901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check is the method has been added during the update (need to run `invoke install` again)\n",
    "# from stemmabench.textual_units import sentence\n",
    "# phrase = sentence.Sentence(\"Phrase\")\n",
    "# dir(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460b1a5a",
   "metadata": {},
   "source": [
    "Create a stemma object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b082d9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m stemma \u001b[39m=\u001b[39m Stemma(original_text\u001b[39m=\u001b[39mDEMO_TEXT, config\u001b[39m=\u001b[39mconfig)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Generate a tradition.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m stemma\u001b[39m.\u001b[39;49mgenerate()\n",
      "File \u001b[1;32mc:\\Users\\Yedidia AGNIMO\\Documents\\BENTO_Internship_2023\\workspaces\\stemmabench\\venv\\lib\\site-packages\\stemmabench\\stemma_generator.py:116\u001b[0m, in \u001b[0;36mStemma.generate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_fragmentation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_text)\n\u001b[0;32m    115\u001b[0m \u001b[39m# Get first variants\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m first_variants \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_level(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moriginal_text)\n\u001b[0;32m    117\u001b[0m \u001b[39m# Append first level\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_levels\u001b[39m.\u001b[39mappend({\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moriginal_text: first_variants})\n",
      "File \u001b[1;32mc:\\Users\\Yedidia AGNIMO\\Documents\\BENTO_Internship_2023\\workspaces\\stemmabench\\venv\\lib\\site-packages\\stemmabench\\stemma_generator.py:106\u001b[0m, in \u001b[0;36mStemma._apply_level\u001b[1;34m(self, manuscript)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply_level\u001b[39m(\u001b[39mself\u001b[39m, manuscript: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m    105\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply transformation on a single generation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_fragmentation(Text(manuscript)\u001b[39m.\u001b[39mtransform(\n\u001b[0;32m    107\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mvariants)) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth)]\n",
      "File \u001b[1;32mc:\\Users\\Yedidia AGNIMO\\Documents\\BENTO_Internship_2023\\workspaces\\stemmabench\\venv\\lib\\site-packages\\stemmabench\\stemma_generator.py:106\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply_level\u001b[39m(\u001b[39mself\u001b[39m, manuscript: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m    105\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply transformation on a single generation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_fragmentation(Text(manuscript)\u001b[39m.\u001b[39mtransform(\n\u001b[0;32m    107\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mvariants)) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth)]\n",
      "File \u001b[1;32mc:\\Users\\Yedidia AGNIMO\\Documents\\BENTO_Internship_2023\\workspaces\\stemmabench\\venv\\lib\\site-packages\\stemmabench\\textual_units\\text.py:26\u001b[0m, in \u001b[0;36mText.__init__\u001b[1;34m(self, text, punc)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initializes an object of class Text, by wrapping a text into it.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39m# FIXME: become more flexible in terms of modelization.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext \u001b[39m=\u001b[39m text\n\u001b[0;32m     25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentences \u001b[39m=\u001b[39m [Sentence(sentence)\n\u001b[1;32m---> 26\u001b[0m                   \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m text\u001b[39m.\u001b[39;49msplit(punc) \u001b[39mif\u001b[39;00m sentence]\n\u001b[0;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwords \u001b[39m=\u001b[39m [Word(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m text\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mif\u001b[39;00m word]\n\u001b[0;32m     28\u001b[0m \u001b[39m# TODO: improve punctuation diversity\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "# Create a stemma object.\n",
    "stemma = Stemma(original_text=DEMO_TEXT, config=config)\n",
    "\n",
    "# Generate a tradition.\n",
    "stemma.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82129c",
   "metadata": {},
   "source": [
    "Each text can be accessed through its lookup table, which can be used to get the tree stemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c80925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemma.texts_lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07960b4",
   "metadata": {},
   "source": [
    "It is also possible to access the edges describing only the manuscript names and their family relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58334fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', '0:0'),\n",
       " ('0', '0:1'),\n",
       " ('0:0', '0:0:0'),\n",
       " ('0:0', '0:0:1'),\n",
       " ('0:1', '0:1:0'),\n",
       " ('0:1', '0:1:1'),\n",
       " ('0:0:0', '0:0:0:0'),\n",
       " ('0:0:0', '0:0:0:1'),\n",
       " ('0:0:1', '0:0:1:0'),\n",
       " ('0:0:1', '0:0:1:1'),\n",
       " ('0:0:0', '0:0:0:0'),\n",
       " ('0:0:0', '0:0:0:1'),\n",
       " ('0:0:1', '0:0:1:0'),\n",
       " ('0:0:1', '0:0:1:1')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemma.edges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
